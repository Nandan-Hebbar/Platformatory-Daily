---------------------------------------------------------------------FileStream Sink and Source connector-----------------------------------------------------------------------------

-->Add the following lines in your etc/kafka/connect-distributed.properties

listeners=http://0.0.0.0:8083
rest.advertised.host.name=localhost
rest.advertised.port=8083
plugin.path=/usr/share/java,/usr/share/filestream-connectors

-->change the file permissions of the input and output file

sudo chmod 777 input.txt
sudo chmod 777 output.txt

-->Enter some data into the input file for testing purpose.

-->Create a topic for testing 

kafka-topics --bootstrap-server localhost:9092 --create --topic connect_topic2 --partitions 1 --replication-factor 1

-->Create a source connector and import the content of the file 

curl -X POST http://localhost:8083/connectors \
-H 'Accept: */*' \
-H 'Content-Type: application/json' \
-d '{
"name": "file_source_connector",
"config": {
"connector.class": "org.apache.kafka.connect.file.FileStreamSourceConnector",
"topic": "connect_topic2",
"file": "/home/nandan/kafka/input.txt",
"value.converter": "org.apache.kafka.connect.storage.StringConverter"
}
}'

-->To check status of the connector

curl http://localhost:8083/connectors/file_source_connector/status

-->To delete the connector

curl -X DELETE http://localhost:8083/connectors/file_source_connector


-->Check the topic for messages.

kafka-console-consumer --bootstrap-server localhost:9092 --topic connect_topic2 --from-beginning

-->Create a sink connector to export data from Kafka to a file.

curl -X POST http://localhost:8083/connectors \
-H 'Accept: */*' \
-H 'Content-Type: application/json' \
-d '{
"name": "file_sink_connector",
"config": {
"connector.class": "org.apache.kafka.connect.file.FileStreamSinkConnector",
"topics": "connect_topic2",
"file": "/home/nandan/kafka/output.txt",
"value.converter": "org.apache.kafka.connect.storage.StringConverter"
}
}'

-->Delete the connectors for clean up



-------------------------------------------------------------------------------------Schema Registry-------------------------------------------------------------------------------------------------
mkdir -p src/main/avro/com/linuxacademy/ccdak/schemaregistry

Create a schema definition for purchases:
nano src/main/avro/com/linuxacademy/ccdak/schemaregistry/Purchase.avsc

{
  "namespace": "com.linuxacademy.ccdak.schemaregistry",
  "type": "record",
  "name": "Purchase",
  "fields": [
    {"name": "id", "type": "int"},
    {"name": "product", "type": "string"},
    {"name": "quantity", "type": "int"}
  ]
}

Implement the producer:

nano src/main/java/com/linuxacademy/ccdak/schemaregistry/ProducerMain.java

package com.linuxacademy.ccdak.schemaregistry;
import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;
import io.confluent.kafka.serializers.KafkaAvroSerializer;
import java.util.Properties;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;
public class ProducerMain {
    public static void main(String[] args) {
        final Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.ACKS_CONFIG, "all");
        props.put(ProducerConfig.RETRIES_CONFIG, 0);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class);
        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, "http://localhost:8081");
        KafkaProducer<String, Purchase> producer = new KafkaProducer<String, Purchase>(props);
        Purchase apples = new Purchase(1, "apples", 17);
        producer.send(new ProducerRecord<String, Purchase>("inventory_purchases", apples.getId().toString(), apples));
        Purchase oranges = new Purchase(2, "oranges", 5);
        producer.send(new ProducerRecord<String, Purchase>("inventory_purchases", oranges.getId().toString(), oranges));
        producer.close();
    }
}

Implement the consumer:

nano src/main/java/com/linuxacademy/ccdak/schemaregistry/ConsumerMain.java

package com.linuxacademy.ccdak.schemaregistry;
import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;
import io.confluent.kafka.serializers.KafkaAvroDeserializer;
import io.confluent.kafka.serializers.KafkaAvroDeserializerConfig;
import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.time.Duration;
import java.util.Collections;
import java.util.Properties;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;
public class ConsumerMain {
    public static void main(String[] args) {
        final Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "group1");
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true");
        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, "1000");
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, "http://localhost:8081");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class);
        props.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, true);
        KafkaConsumer<String, Purchase> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList("inventory_purchases"));
        try {
            BufferedWriter writer = new BufferedWriter(new FileWriter("/home/mpshriveena/Desktop/Platformatory/trials/output.txt", true));
            while (true) {
                final ConsumerRecords<String, Purchase> records = consumer.poll(Duration.ofMillis(100));
                for (final ConsumerRecord<String, Purchase> record : records) {
                    final String key = record.key();
                    final Purchase value = record.value();
                    String outputString = "key=" + key + ", value=" + value;
                    System.out.println(outputString);
                    writer.write(outputString + "\n");
                }
                writer.flush();
            }
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }
}
Run the producer:
./gradlew runProducer

Run the consumer:
./gradlew runConsumer

Verify the data in the output file:
cat /home/mpshriveena/Desktop/Platformatory/trials/output.txt






