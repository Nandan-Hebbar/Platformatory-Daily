---------------------------------------------------------------------FileStream Sink and Source connector-----------------------------------------------------------------------------

-->Add the following lines in your etc/kafka/connect-distributed.properties

listeners=http://0.0.0.0:8083
rest.advertised.host.name=localhost
rest.advertised.port=8083
plugin.path=/usr/share/java,/usr/share/filestream-connectors

-->change the file permissions of the input and output file

sudo chmod 777 input.txt
sudo chmod 777 output.txt

-->Enter some data into the input file for testing purpose.

-->Create a topic for testing 

kafka-topics --bootstrap-server localhost:9092 --create --topic connect_topic2 --partitions 1 --replication-factor 1

-->Create a source connector and import the content of the file 

curl -X POST http://localhost:8083/connectors 
-H 'Accept: */*' -H 'Content-Type: application/json' -d '{
"name": "file_source_connector",
"config": {
"connector.class": "org.apache.kafka.connect.file.FileStreamSourceConnector",
"topic": "connect_topic_test",
"file": "/home/nandan/kafkafiles/input.txt",
"value.converter": "org.apache.kafka.connect.storage.StringConverter"
}
}'

-->To check status of the connector

curl http://localhost:8083/connectors/file_source_connector/status

-->To delete the connector

curl -X DELETE http://localhost:8083/connectors/file_source_connector


-->Check the topic for messages.

kafka-console-consumer --bootstrap-server localhost:9092 --topic connect_topic2 --from-beginning

-->Create a sink connector to export data from Kafka to a file.

curl -X POST http://localhost:8083/connectors \
-H 'Accept: */*' \
-H 'Content-Type: application/json' \
-d '{
"name": "file_sink_connector",
"config": {
"connector.class": "org.apache.kafka.connect.file.FileStreamSinkConnector",
"topics": "connect_topic_test",
"file": "/home/nandan/kafkafiles/output.txt",
"value.converter": "org.apache.kafka.connect.storage.StringConverter"
}
}'

-->Delete the connectors for clean up



------------------------------------------------------------------------------------JDBC Connector--------------------------------------------------------------------------------------------
-->Run
sudo pg_ctlcluster 12 main start

--> To login into psql console
sudo -u postgres psql

--> To create a Database
CREATE DATABASE inventory3;

-->To list all the databases with its ownership details 
\l 

-->To Move inside it
\c inventory3

-->Create table inside inventory database
CREATE TABLE purchases (
    id SERIAL PRIMARY KEY,
    item_name VARCHAR(100),
    quantity INT,
    price DECIMAL(10, 2),
    update_ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-->Insert Data
INSERT INTO purchases (item_name, quantity, price) VALUES ("   ",   ,   );

SELECT * FROM purchases;

-->Exit from psql console
\q


JDBC Connector format

curl -X POST http://localhost:8083/connectors -H "Content-Type: application/json" -d '{
"name": "jdbc_source_postgres",
"config": {
"connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
"connection.url": "jdbc:postgresql://127.0.0.1:5432/inventory3",
"connection.user": "postgres",
"connection.password": "729999",
"topic.prefix": "postgresql-",
"mode":"timestamp",
"timestamp.column.name": "update_ts"
}
}'




-------------------------------------------------------------------------Schema Registry with producer and consumer in Java-------------------------------------------------------------------------------------------------


sudo mkdir -p src/main/avro/com/linuxacademy/ccdak/schemaregistry

-->Create a schema for purchases:
sudo vi src/main/avro/com/linuxacademy/ccdak/schemaregistry/Purchase.avsc

{
  "namespace": "com.linuxacademy.ccdak.schemaregistry",
  "type": "record",
  "name": "Purchase",
  "fields": [
    {"name": "id", "type": "int"},
    {"name": "product", "type": "string"},
    {"name": "quantity", "type": "int"}
  ]
}

-->Implementation the producer:

sudo vi src/main/java/com/linuxacademy/ccdak/schemaregistry/ProducerMain.java

package com.linuxacademy.ccdak.schemaregistry;

import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;
import io.confluent.kafka.serializers.KafkaAvroSerializer;
import java.util.Properties;
import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerConfig;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.common.serialization.StringSerializer;

public class ProducerMain {

    public static void main(String[] args) {
        final Properties props = new Properties();
        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ProducerConfig.ACKS_CONFIG, "all");
        props.put(ProducerConfig.RETRIES_CONFIG, 0);
        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class);
        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, KafkaAvroSerializer.class);
        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, "http://localhost:8081");

        KafkaProducer<String, Purchase> producer = new KafkaProducer<String, Purchase>(props);

        Purchase apples = new Purchase(1, "apples", 17, 9);
        producer.send(new ProducerRecord<String, Purchase>("schregdemo", apples.getId().toString(), apples));

        Purchase oranges = new Purchase(2, "oranges", 5, 8);
        producer.send(new ProducerRecord<String, Purchase>("schregdemo", oranges.getId().toString(), oranges));

        producer.close();

    }

}

-->Implementation of the consumer:

sudo vi src/main/java/com/linuxacademy/ccdak/schemaregistry/ConsumerMain.java

package com.linuxacademy.ccdak.schemaregistry;

import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;
import io.confluent.kafka.serializers.KafkaAvroDeserializer;
import io.confluent.kafka.serializers.KafkaAvroDeserializerConfig;
import java.io.BufferedWriter;
import java.io.FileWriter;
import java.io.IOException;
import java.time.Duration;
import java.util.Collections;
import java.util.Properties;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.common.serialization.StringDeserializer;

public class ConsumerMain {

    public static void main(String[] args) {
        final Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "group1");
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, "true");
        props.put(ConsumerConfig.AUTO_COMMIT_INTERVAL_MS_CONFIG, "1000");
        props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, "http://localhost:8081");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class);
        props.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, true);

        KafkaConsumer<String, Purchase> consumer = new KafkaConsumer<>(props);

        consumer.subscribe(Collections.singletonList("schregdemo"));

        try {
            BufferedWriter writer = new BufferedWriter(new FileWriter("/home/nandan/schregoutput.txt", true));
            while (true) {
                final ConsumerRecords<String, Purchase> records = consumer.poll(Duration.ofMillis(100));
                for (final ConsumerRecord<String, Purchase> record : records) {
                    final String key = record.key();
                    final Purchase value = record.value();
                    String outputString = "key=" + key + ", value=" + value;
                    System.out.println(outputString);
                    writer.write(outputString + "\n");
                }
                writer.flush();
            }
        } catch (IOException e) {
            throw new RuntimeException(e);
        }
    }

}


-->Run the producer:
./gradlew runProducer

-->Run the consumer:
./gradlew runConsumer

-->Verify the data in the output file:
cat /home/nandan/schregoutput.txt


-->Managing changes in the avro schema

Compatibility type
1. Backward(Default)                                      
2. Backward Transitive                                 
3. Forward                                             
4. Forward Transitive                                 
5. Full                                                                        
6. Full Transitive                                                           
7. None                            

-->Backward Compatibility

New schemas are compatible with data written by old schemas.
Consumers using the new schema can read data written with any previous version of the schema.
Use Case: Ensures newer consumers can handle older data.
Example: Adding a new optional field or removing an unused field.

Schema Evolution Rules:

Fields can be added with a default value.
Fields can be removed if not required.



-->Backward Transitive Compatibility
New schemas are compatible with all previous versions of schemas (not just the last one).
Use Case: Ensures that a schema evolves without breaking compatibility with any prior schema version.


Schema Evolution Rules:

Same as backward and forward compatibility.



-->Forward Compatibility

Old schemas are compatible with data written by new schemas.
Consumers using the old schema can read data written with the new schema.
Use Case: Ensures older consumers can process data written using newer schemas.
Example: Adding a new field with a default value.

Schema Evolution Rules:

Fields can be added with default values.
Fields can be removed.




-->Forward Transitive Compatibility
Old schemas are compatible with all future versions of schemas.
Use Case: Ensures that data written with the current schema can be read by all future schema versions.




-->Full Compatibility

Combines both backward and forward compatibility.
Both old and new schemas can read data written by each other.
Use Case: Maximum flexibility for schema evolution while ensuring interoperability.
Example: Adding an optional field with a default value.





-->Full Transitive Compatibility
Combines backward transitive and forward transitive compatibility.
Use Case: Guarantees maximum compatibility across all schema versions.


-->None
No compatibility checks are enforced.
Use Case: Useful during development or when compatibility is not a concern. However, this setting risks breaking consumers.

-->Goto sudo vi src/main/avro/com/linuxacademy/ccdak/schemaregistry/Purchase.avsc and add another field Discount and compatibility type

{
  "namespace": "com.linuxacademy.ccdak.schemaregistry",
  "compatibility":"BACKWARD",
  "type": "record",
  "name": "Purchase",
  "fields": [
    {"name": "id", "type": "int"},
    {"name": "product", "type": "string"},
    {"name": "quantity", "type": "int"},
    {"name": "discount", "type": "int", "default": ""}
  ]
}

In the producer add the discount field in the constructor 
There is no need to make changes in the consumer because there is no explicit reference made to it and it is simply printing the corresponding key value pair





--------------------------------------------------------------------------------------REST API----------------------------------------------------------------------------------------
sudo apt-get install confluent-rest

-->Create a topic resttopic
kafka-topics --bootstrap-server localhost:9092 --topic resttopic2 --create --partitions 1 --replication-factor 1

sudo systemctl start confluent-schema-registry confluent-kafka-rest
sudo systemctl status confluent-schema-registry confluent-kafka-rest

1) Producing messages

-curl X POST -H "Content-Type: application/vnd.kafka.json.v2+json" \
-H "Accept: application/vnd.kafka.v2+json" \
--data '{"records":[{"key":"apples","value":"23"},{"key":"grapes","value":"160"}]}' \
"http://localhost:8082/topics/resttopic3"

2) To create a Consumer and Consumer instance

curl -X POST -H "Content-Type: application/vnd.kafka.v2+json" \
  --data '{"name": "resttopic3consumerinstance", "format": "json", "auto.offset.reset": "earliest"}' \
  http://localhost:8082/consumers/resttopic3consumer

--Output will look similar to thiis

{"instance_id":"resttopic2consumer","base_uri":"http://localhost:8082/consumers/resttopic2consumer/instances/resttopic2consumerinstance"} 

the output base uri will match the uri in the subscription and the GET command


3) Subscribe to the topic

curl -X POST -H "Content-Type: application/vnd.kafka.v2+json" --data '{"topics":["resttopic3"]}' \
  http://localhost:8082/consumers/resttopic3consumer/instances/resttopic2consumerinstance/subscription

4) To consume 

curl -X GET -H "Accept: application/vnd.kafka.json.v2+json" \
  http://localhost:8082/consumers/resttopic2consumer/instances/resttopic3consumerinstance/records



---------------------------------------------------------------------------------------Maven-------------------------------------------------------------------------------------------------

 project-root/
│
├── src/
│   ├── main/
│   │   ├── java/                     # Application/Library source code
│   │   │   └── com/
│   │   │       └── example/
│   │   │           └── MyClass.java
│   │   ├── resources/                # Configuration files, static resources
│   │       └── application.properties
│   │
│   ├── test/
│       ├── java/                     # Test source code
│       │   └── com/
│       │       └── example/
│       │           └── MyClassTest.java
│       ├── resources/                # Test-specific resources
│           └── test-config.properties
│
├── target/                           # Compiled bytecode, JAR files, and build output (generated after building)
│   ├── classes/
│   ├── test-classes/
│   ├── myproject-1.0.jar
│
├── pom.xml                           # Project Object Model file (Maven configuration)



Maven installation

1. Update the Package Repository:
sudo apt update

2. Install Maven:
sudo apt install maven

3. Verify installation
mvn -version



-->To create a Maven project

mvn archetype:generate -DgroupId=com.evenapp -DartifactId=even-number-app -DarchetypeArtifactId=maven-archetype-quickstart -DinteractiveMode=false


-->To compile maven project

mvn compile exec:java -Dexec.mainClass="com.example.JavaProducer"


-->To build the project

mvn clean package


-->To run the application

java -cp target/even-number-app-1.0-SNAPSHOT.jar com.evenapp.App


-->To manually reset the offset to 0

kafka-consumer-groups --bootstrap-server localhost:9092 --group even-number-group --reset-offsets --to-earliest --execute --topic even-numbers


-->To run Producer/Consumer alone

mvn compile exec:java -Dexec.mainClass="com.example.EvenNumberProducer"
mvn compile exec:java -Dexec.mainClass="com.example.EvenNumberConsumer"


-->Add the dependencies, for every project you might have to add dependencies in pom.xml file




-->Task 1:
Write a producer to print all even numbers from 1 to 1000 and then consume it using java prod and consumer using Maven

-->EvenNumberProducer.java

package com.example;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;

import java.util.Properties;

public class EvenNumberProducer {
    public static void main(String[] args) {
        String topic = "even-numbers";

        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        KafkaProducer<String, String> producer = new KafkaProducer<>(props);

        for (int i = 2; i <= 1000; i += 2) {
            producer.send(new ProducerRecord<>(topic, Integer.toString(i)));
            System.out.println("Produced: " + i);
        }

        producer.close();
    }
}


-->EvenNumberConsumer.java

package com.example;

import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.ConsumerRecord;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class EvenNumberConsumer {
    public static void main(String[] args) {
        String topic = "even-numbers";

        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("group.id", "even-number-group");
	props.put("auto.offset.reset","earliest");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList(topic));

        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
            for (ConsumerRecord<String, String> record : records) {
                System.out.println("Consumed: " + record.value());
            }
        }
    }
}


--Task 2:
Write java producer to produce json data from a file and then consume it using java consumer using Maven


-->Create a json file under /src/main/resources directory with the following structure that is the input for the kafka topic

[
  {"id": 1, "name": "Dave"},
  {"id": 2, "name": "Damon"},
  {"id": 3, "name": "Mark"}
]


-->Producer code

package com.example;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;

import java.io.IOException;
import java.nio.file.Files;
import java.nio.file.Paths;
import java.util.Properties;

public class JsonProducer {
    public static void main(String[] args) throws IOException {
        String topic = "json-topic";

        // Kafka Producer Properties
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        props.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");

        KafkaProducer<String, String> producer = new KafkaProducer<>(props);

        // Read JSON data from file
        String filePath = "src/main/resources/inputfile.json";
        String jsonData = new String(Files.readAllBytes(Paths.get(filePath)));

        // Send JSON data to Kafka
        producer.send(new ProducerRecord<>(topic, "json_key", jsonData));
        System.out.println("JSON data sent to topic: " + topic);

        producer.close();
    }
}




-->Consumer code

package com.example;

import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.ConsumerRecord;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class JsonConsumer {
    public static void main(String[] args) {
        String topic = "json-topic";

        // Kafka Consumer Properties
        Properties props = new Properties();
        props.put("bootstrap.servers", "localhost:9092");
        props.put("group.id", "json-consumer-group");
        props.put("auto.offset.reset", "earliest");
        props.put("key.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");
        props.put("value.deserializer", "org.apache.kafka.common.serialization.StringDeserializer");

        KafkaConsumer<String, String> consumer = new KafkaConsumer<>(props);

        // Subscribe to topic
        consumer.subscribe(Collections.singletonList(topic));

        // Consume messages
        while (true) {
            ConsumerRecords<String, String> records = consumer.poll(Duration.ofMillis(100));
            for (ConsumerRecord<String, String> record : records) {
                System.out.println("Consumed JSON: " + record.value());
            }
        }
    }
}



-->Task 3:

Write a java consumer to consume avro data, generate the avro data using a datagen connector using Maven




sudo systemctl start confluent-zookeeper
sudo systemctl start confluent-kafka
sudo systemctl start confluent-kafka-connect
sudo systemctl start confluent-kafka-rest
sudo systemctl start confluent-schema-registry



kafka-topics --create --topic avrotopic3 --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1

-->under src/main/resources/avro/ place your user.avsc with the following format

{
  "type": "record",
  "name": "User",
  "fields": [
    {"name": "id", "type": "int"},
    {"name": "name", "type": "string"},
    {"name": "age", "type": "int"},
    {"name": "email", "type": "string"}
  ]
}


-->Run the Maven command to generate the Avro Java classes:

mvn generate-sources


-->Create an output directory under target/generated-sources 

-->In the home directory of the app create a json file to store the connector configuration and place the code below

{
  "name": "datagen-connector3",
  "config": {
    "connector.class": "io.confluent.kafka.connect.datagen.DatagenConnector",
    "kafka.topic": "avrotopic3",
    "quickstart": "users",
    "key.converter": "org.apache.kafka.connect.storage.StringConverter",
    "value.converter": "io.confluent.connect.avro.AvroConverter",
    "value.converter.schema.registry.url": "http://localhost:8081",
    "tasks.max": "1"
  }
}




-->Submit the connector Configuration

 Use the Kafka Connect REST API to submit the configuration

  Run the following curl command to post the configuration to your Kafka Connect instance:

curl -X POST -H "Content-Type: application/json" --data @datagen-connector-config.json http://localhost:8083/connectors




-->Check status of the connector and see if thats running

curl -X GET http://localhost:8083/connectors/datagen-connector/status



-->Consumer code

package com.avroapp;

import io.confluent.kafka.serializers.KafkaAvroDeserializer;
import org.apache.kafka.clients.consumer.Consumer;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.clients.consumer.KafkaConsumer;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class AvroConsumer {

    public static void main(String[] args) {
        String topic = "avrotopic3";
        String bootstrapServers = "localhost:9092";
        String schemaRegistryUrl = "http://localhost:8081";

        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "avro-consumers-group");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, "org.apache.kafka.common.serialization.StringDeserializer");
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class.getName());
        props.put("auto.offset.reset", "earliest");
        props.put("schema.registry.url", schemaRegistryUrl);

        Consumer<String, Object> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Collections.singletonList(topic));

        System.out.println("Listening for Avro messages on topic: " + topic);

        try {
            while (true) {
                ConsumerRecords<String, Object> records = consumer.poll(Duration.ofMillis(1000));
                records.forEach(record -> {
                    System.out.printf("Key: %s, Value: %s%n", record.key(), record.value());
                });
            }
        } catch (Exception e) {
            e.printStackTrace();
        } finally {
            consumer.close();
        }
    }
}




-->Equivalent python code for this


from confluent_kafka import Consumer
from confluent_kafka.avro import AvroConsumer
from confluent_kafka.avro.serializer import SerializerError


def main():
    topic = "avrotopic3"
    bootstrap_servers = "localhost:9092"
    schema_registry_url = "http://localhost:8081"

    # Consumer configuration
    consumer_config = {
        'bootstrap.servers': bootstrap_servers,
        'group.id': 'avro-consumers-group',
        'auto.offset.reset': 'earliest',
        'schema.registry.url': schema_registry_url
    }

    # Create Avro consumer
    consumer = AvroConsumer(consumer_config)
    consumer.subscribe([topic])

    print(f"Listening for Avro messages on topic: {topic}")

    try:
        while True:
            try:
                # Poll for messages
                message = consumer.poll(timeout=1.0)

                if message is None:
                    continue

                if message.error():
                    print(f"Consumer error: {message.error()}")
                    continue

                # Print the message key and value
                print(f"Key: {message.key()}, Value: {message.value()}")

            except SerializerError as e:
                print(f"Message deserialization failed: {e}")
                continue

    except KeyboardInterrupt:
        print("Aborted by user")

    finally:
        # Close the consumer
        consumer.close()


if __name__ == "__main__":
    main()



-->Clean and compile the project to ensure the package structure is correct

mvn clean compile


-->Run the project 

mvn exec:java -Dexec.mainClass="com.avroapp.AvroConsumer"



-->You should see the randomly generated output referring to your schema like

Listening for Avro messages on topic: avro_topic
Key: User_7, Value: {"registertime": 1498111634110, "userid": "User_7", "regionid": "Region_7", "gender": "MALE"}
Key: User_4, Value: {"registertime": 1508596987426, "userid": "User_4", "regionid": "Region_4", "gender": "MALE"}
Key: User_3, Value: {"registertime": 1491855539024, "userid": "User_3", "regionid": "Region_8", "gender": "OTHER"}
Key: User_4, Value: {"registertime": 1511737278946, "userid": "User_4", "regionid": "Region_2", "gender": "FEMALE"}
Key: User_1, Value: {"registertime": 1510893814669, "userid": "User_1", "regionid": "Region_3", "gender": "OTHER"}
Key: User_5, Value: {"registertime": 1496154112225, "userid": "User_5", "regionid": "Region_6", "gender": "MALE"}
Key: User_3, Value: {"registertime": 1488223068497, "userid": "User_3", "regionid": "Region_7", "gender": "OTHER"}




------------------------------------------------------------------------------Docker compose with 3 kafka brokers-----------------------------------------------------------------------

To open bash

sudo docker-compose exec --user root kafka-1 /bin/bash


kafka-topics --list --bootstrap-server kafka1:19092 --command-config /opt/client/client.properties



version: '3.7'
services:
  zookeeper:
    image: 'confluentinc/cp-zookeeper'
    container_name: zookeeper
    ports:
      - '2182:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    restart: always
  kafka-1:
    image: 'confluentinc/cp-server:latest'
    container_name: kafka-1
    depends_on:
      - zookeeper
    ports:
      - '9094:9094'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9094,INTERNAL://kafka-1:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
    volumes:
      - kafka-1-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs     
    restart: always
  kafka-2:
    image: 'confluentinc/cp-server:latest'
    container_name: kafka-2
    depends_on:
      - zookeeper
      - kafka-1
    ports:
      - '9095:9095'
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9095,INTERNAL://kafka-2:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
    volumes:
      - kafka-2-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs
    restart: always
  kafka-3:
    image: 'confluentinc/cp-server:latest'
    container_name: kafka-3
    depends_on:
      - zookeeper
      - kafka-1
      - kafka-2
    ports:
      - '9096:9096'
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9096,INTERNAL://kafka-3:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
    volumes:
      - kafka-3-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs
    restart: always
volumes:
  kafka-1-data:
  kafka-2-data:
  kafka-3-data:
      
      
      



-----------------------------------------------------------------------------SSL/TLS on docker compose--------------------------------------------------------------------------------

Implementation of TLS-SSL in Docker Compose

cd /etc/kafka/secrets

1) CA Certificate
openssl req -new -x509 -keyout ca-key.pem -out ca-cert.pem -days 365 -subj "/C=IN/ST=Karnataka/L=Uttarahalli/O=Platformatory/OU=Kafka/CN=Platformatory-CA"

2) Client-1 Keystore and Truststore
a. keytool -keystore client-1.keystore.jks -alias kafka1 -validity 365 -genkey -keyalg RSA -dname "CN=kafka1, OU=Kafka, O=Platformatory, L=Uttarahalli, ST=Karnataka, C=IN"
b. keytool -keystore client-1.keystore.jks -alias kafka1 -certreq -file client-1-cert-file
c. openssl x509 -req -CA ca-cert.pem -CAkey ca-key.pem -in client-1-cert-file -out client-1-cert-signed.pem -days 365 -CAcreateserial
d. keytool -keystore client-1.keystore.jks -alias CARoot -import -file ca-cert.pem
e. keytool -keystore client-1.keystore.jks -alias kafka1 -import -file client-1-cert-signed.pem
f. openssl verify -CAfile ca-cert.pem client-1-cert-signed.pem
#client-cert-signed.pem: OK
g. keytool -importcert -file ca-cert.pem -alias CARoot -keystore client1.truststore.jks

3) Kafka-1 Keystore and Truststore
a. keytool -keystore kafka1.keystore.jks -alias kafka1 -validity 365 -genkey -keyalg RSA -dname "CN=kafka1, OU=Kafka, O=Platformatory, L=Uttarahalli, ST=Karnataka, C=IN"
b. keytool -keystore kafka1.keystore.jks -alias kafka1 -certreq -file kafka1-cert-file
c. openssl x509 -req -CA ca-cert.pem -CAkey ca-key.pem -in kafka1-cert-file -out kafka1-cert-signed.pem -days 365 -CAcreateserial
d. keytool -keystore kafka1.keystore.jks -alias CARoot -import -file ca-cert.pem
e. keytool -keystore kafka1.keystore.jks -alias kafka1 -import -file kafka1-cert-signed.pem
f. openssl verify -CAfile ca-cert.pem kafka1-cert-signed.pem
#kafka-1-cert-signed.pem: OK
g. keytool -importcert -file ca-cert.pem -alias CARoot -keystore kafka1.truststore.jks

4) Client-2 Keystore and Truststore
keytool -keystore client-2.keystore.jks -alias kafka2 -validity 365 -genkey -keyalg RSA -dname "CN=kafka2, OU=Kafka, O=Platformatory, L=Uttarahalli, ST=Karnataka, C=IN"
keytool -keystore client-2.keystore.jks -alias kafka2 -certreq -file client-2-cert-file
openssl x509 -req -CA ca-cert.pem -CAkey ca-key.pem -in client-2-cert-file -out client-2-cert-signed.pem -days 365 -CAcreateserial
keytool -keystore client-2.keystore.jks -alias CARoot -import -file ca-cert.pem
keytool -keystore client-2.keystore.jks -alias kafka2 -import -file client-2-cert-signed.pem
openssl verify -CAfile ca-cert.pem client-2-cert-signed.pem
#client-cert-signed.pem: OK
keytool -importcert -file ca-cert.pem -alias CARoot -keystore client-2.truststore.jks

5) Kafka-2 Keystore and Truststore
keytool -keystore kafka2.keystore.jks -alias kafka2 -validity 365 -genkey -keyalg RSA -dname "CN=kafka2, OU=Kafka, O=Platformatory, L=Uttarahalli, ST=Karnataka, C=IN"
keytool -keystore kafka2.keystore.jks -alias kafka2 -certreq -file kafka2-cert-file
openssl x509 -req -CA ca-cert.pem -CAkey ca-key.pem -in kafka2-cert-file -out kafka2-cert-signed.pem -days 365 -CAcreateserial
keytool -keystore kafka2.keystore.jks -alias CARoot -import -file ca-cert.pem
keytool -keystore kafka2.keystore.jks -alias kafka2 -import -file kafka2-cert-signed.pem
openssl verify -CAfile ca-cert.pem kafka2-cert-signed.pem
#kafka-2-cert-signed.pem: OK
keytool -importcert -file ca-cert.pem -alias CARoot -keystore kafka2.truststore.jks

6) Client-3 Keystore and Truststore
keytool -keystore client-3.keystore.jks -alias kafka3 -validity 365 -genkey -keyalg RSA -dname "CN=kafka3, OU=Kafka, O=Platformatory, L=Uttarahalli, ST=Karnataka, C=IN"
keytool -keystore client-3.keystore.jks -alias kafka3 -certreq -file client-3-cert-file
openssl x509 -req -CA ca-cert.pem -CAkey ca-key.pem -in client-3-cert-file -out client-3-cert-signed.pem -days 365 -CAcreateserial
keytool -keystore client-3.keystore.jks -alias CARoot -import -file ca-cert.pem
keytool -keystore client-3.keystore.jks -alias kafka3 -import -file client-3-cert-signed.pem
openssl verify -CAfile ca-cert.pem client-3-cert-signed.pem
#client-cert-signed.pem: OK
keytool -importcert -file ca-cert.pem -alias CARoot -keystore client-3.truststore.jks

7) Kafka-3 Keystore and Truststore
keytool -keystore kafka3.keystore.jks -alias kafka3 -validity 365 -genkey -keyalg RSA -dname "CN=kafka3, OU=Kafka, O=Platformatory, L=Uttarahalli, ST=Karnataka, C=IN"
keytool -keystore kafka3.keystore.jks -alias kafka3 -certreq -file kafka3-cert-file
openssl x509 -req -CA ca-cert.pem -CAkey ca-key.pem -in kafka3-cert-file -out kafka3-cert-signed.pem -days 365 -CAcreateserial
keytool -keystore kafka3.keystore.jks -alias CARoot -import -file ca-cert.pem
keytool -keystore kafka3.keystore.jks -alias kafka3 -import -file kafka3-cert-signed.pem
openssl verify -CAfile ca-cert.pem kafka3-cert-signed.pem
#kafka-3-cert-signed.pem: OK
keytool -importcert -file ca-cert.pem -alias CARoot -keystore kafka3.truststore.jks

7) Create Credentials for docker compose file
echo "729999" > ./keystore-password.txt
echo "729999" > ./key-password.txt
echo "729999" > ./truststore-password.txt

cd /etc/kafka/configs

8) Create Client-1 SSL Properties
echo "security.protocol=SSL" >> client-1-ssl.properties
echo "ssl.truststore.location=/etc/kafka/secrets/client-1.truststore.jks" >> client-1-ssl.properties
echo "ssl.truststore.password=729999" >> client-1-ssl.properties
echo "ssl.keystore.location=/etc/kafka/secrets/client-1.keystore.jks" >> client-1-ssl.properties
echo "ssl.keystore.password=729999" >> client-1-ssl.properties
echo "ssl.key.password=729999" >> client-1-ssl.properties
echo "ssl.enabled.protocols=TLSv1.2,TLSv1.3" >> client-1-ssl.properties
echo "ssl.client.auth=required" >> client-1-ssl.properties

9) Create Client-2 SSL Properties
echo "security.protocol=SSL" >> client-2-ssl.properties
echo "ssl.truststore.location=/etc/kafka/secrets/client-2.truststore.jks" >> client-2-ssl.properties
echo "ssl.truststore.password=729999" >> client-2-ssl.properties
echo "ssl.keystore.location=/etc/kafka/secrets/client-2.keystore.jks" >> client-2-ssl.properties
echo "ssl.keystore.password=729999" >> client-2-ssl.properties
echo "ssl.key.password=729999" >> client-2-ssl.properties
echo "ssl.enabled.protocols=TLSv1.2,TLSv1.3" >> client-2-ssl.properties
echo "ssl.client.auth=required" >> client-2-ssl.properties

10) Create Client-3 SSL Properties
echo "security.protocol=SSL" >> client-3-ssl.properties
echo "ssl.truststore.location=/etc/kafka/secrets/client-3.truststore.jks" >> client-3-ssl.properties
echo "ssl.truststore.password=729999" >> client-3-ssl.properties
echo "ssl.keystore.location=/etc/kafka/secrets/client-3.keystore.jks" >> client-3-ssl.properties
echo "ssl.keystore.password=729999" >> client-3-ssl.properties
echo "ssl.key.password=729999" >> client-3-ssl.properties
echo "ssl.enabled.protocols=TLSv1.2,TLSv1.3" >> client-3-ssl.properties
echo "ssl.client.auth=required" >> client-3-ssl.properties




version: '3.7'
services:
  zookeeper:
    image: 'confluentinc/cp-zookeeper'
    container_name: zookeeper
    ports:
      - '2182:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
    restart: always

  kafka-1:
    image: 'confluentinc/cp-server:latest'
    container_name: kafka-1
    depends_on:
      - zookeeper
    ports:
      - '9094:9094'
      - '9097:9097'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_JVM_PERFORMANCE_OPTS: "-Xms1G -Xmx2G"
      KAFKA_MAX_MESSAGE_BYTES: 400000000
      KAFKA_REPLICA_FETCH_MAX_BYTES: 400000000
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9094,INTERNAL://kafka-1:9092,SSL://kafka-1:9097
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9094,INTERNAL://0.0.0.0:9092,SSL://0.0.0.0:9097
      KAFKA_SSL_KEYSTORE_FILENAME: kafka-1.keystore.jks
      KAFKA_SSL_KEY_CREDENTIALS: key-password.txt
      KAFKA_SSL_KEYSTORE_CREDENTIALS: keystore-password.txt
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: truststore-password.txt
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka-1.truststore.jks
      KAFKA_SSL_CLIENT_AUTH: 'required'
    volumes:
      - kafka-1-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs
    restart: always

  kafka-2:
    image: 'confluentinc/cp-server:latest'
    container_name: kafka-2
    depends_on:
      - zookeeper
      - kafka-1
    ports:
      - '9095:9095'
      - '9098:9098'
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_JVM_PERFORMANCE_OPTS: "-Xms1G -Xmx2G"
      KAFKA_MAX_MESSAGE_BYTES: 400000000
      KAFKA_REPLICA_FETCH_MAX_BYTES: 400000000
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9095,INTERNAL://kafka-2:9092,SSL://kafka-2:9097
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      LISTENERS: PLAINTEXT://0.0.0.0:9095,INTERNAL://0.0.0.0:9092,SSL://0.0.0.0:9097
      KAFKA_SSL_KEYSTORE_FILENAME: kafka-2.keystore.jks
      KAFKA_SSL_KEY_CREDENTIALS: key-password.txt
      KAFKA_SSL_KEYSTORE_CREDENTIALS: keystore-password.txt
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: truststore-password.txt
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka-2.truststore.jks
      KAFKA_SSL_CLIENT_AUTH: 'required'
    volumes:
      - kafka-2-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs
    restart: always

  kafka-3:
    image: 'confluentinc/cp-server:latest'
    container_name: kafka-3
    depends_on:
      - zookeeper
      - kafka-1
      - kafka-2
    ports:
      - '9096:9096'
      - '9099:9099'
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_JVM_PERFORMANCE_OPTS: "-Xms1G -Xmx2G"
      KAFKA_MAX_MESSAGE_BYTES: 400000000
      KAFKA_REPLICA_FETCH_MAX_BYTES: 400000000
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9096,INTERNAL://kafka-3:9092,SSL://kafka-3:9097
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT,SSL:SSL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      LISTENERS: PLAINTEXT://0.0.0.0:9096,INTERNAL://0.0.0.0:9092,SSL://0.0.0.0:9097
      KAFKA_SSL_KEYSTORE_FILENAME: kafka-3.keystore.jks
      KAFKA_SSL_KEY_CREDENTIALS: key-password.txt
      KAFKA_SSL_KEYSTORE_CREDENTIALS: keystore-password.txt
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: truststore-password.txt
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka-3.truststore.jks
      KAFKA_SSL_CLIENT_AUTH: 'required'
    volumes:
      - kafka-3-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs
    restart: always

volumes:
  kafka-1-data:
  kafka-2-data:
  kafka-3-data:


-->Test by creating a test topic and produce some message and consume using the below command

kafka-topics --create --topic ssltls --bootstrap-server kafka-1:9092 --partitions 1 --replication-factor 1

kafka-topics --list --bootstrap-server kafka-1:9092




-->Produce some message into the topic 

kafka-console-producer --topic ssltls --bootstrap-server kafka-1:9097 --producer.config /etc/kafka/secrets/client-1-ssl.properties



-->Consume the produced message

kafka-console-consumer --bootstrap-server kafka-1:9097 --topic ssltls --from-beginning --consumer.config /etc/kafka/secrets/client-1-ssl.properties



-----------------------------------------------------------------------SASL plaintext---------------------------------------------------------------------------



Implementation of SASL plaintext in Docker Compose

cd /etc/kafka/configs

1) Create Client Properties
echo "security.protocol=SASL_PLAINTEXT" > client-sasl.properties
echo "sasl.mechanism=PLAIN" >> client-sasl.properties
echo "sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\" >> client-sasl.properties
echo "    username=\"nandan\" \\" >> client-sasl.properties
echo "    password=\"729999\";" >> client-sasl.properties
echo "max.request.size=209715200" >> client-sasl.properties

2) Create Server JAAS Configuration
echo "KafkaServer {" >> kafka_server_jaas.conf
echo "    org.apache.kafka.common.security.plain.PlainLoginModule required" >> kafka_server_jaas.conf
echo "    username=\"nandan\"" >> kafka_server_jaas.conf
echo "    password=\"729999\"" >> kafka_server_jaas.conf
echo "    user_nandan=\"729999\";" >> kafka_server_jaas.conf
echo "};" >> kafka_server_jaas.conf





version: '3.7'
services:
  zookeeper:
    image: 'confluentinc/cp-zookeeper'
    container_name: zookeeper
    ports:
      - '2182:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - /etc/kafka/data/zookeeper_data:/var/lib/zookeeper/data
      - /etc/kafka/data/zookeeper_log:/var/lib/zookeeper/log
    restart: always

  kafka-1:
    image: 'confluentinc/cp-server:latest'
    container_name: kafka-1
    depends_on:
      - zookeeper
    ports:
      - '9094:9094'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      ZOOKEEPER_SASL_ENABLED: 'false'
      KAFKA_JVM_PERFORMANCE_OPTS: "-Xms1G -Xmx2G"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9094,INTERNAL://kafka-1:9092,SASL_PLAINTEXT://kafka-1:9098
      KAFKA_SECURITY_INTER_BROKER_PROTOCOL: SASL_PLAINTEXT
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT,SASL_PLAINTEXT:SASL_PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9094,INTERNAL://0.0.0.0:9092,SASL_PLAINTEXT://0.0.0.0:9098
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/configs/kafka_server_jaas.conf"
    volumes:
      - kafka-1-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs
    restart: always

  kafka-2:
    image: 'confluentinc/cp-server:latest'
    container_name: kafka-2
    depends_on:
      - zookeeper
      - kafka-1
    ports:
      - '9095:9095'
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      ZOOKEEPER_SASL_ENABLED: 'false'
      KAFKA_JVM_PERFORMANCE_OPTS: "-Xms1G -Xmx2G"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9095,INTERNAL://kafka-2:9092,SASL_PLAINTEXT://kafka-2:9098
      KAFKA_SECURITY_INTER_BROKER_PROTOCOL: SASL_PLAINTEXT
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT,SASL_PLAINTEXT:SASL_PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9095,INTERNAL://0.0.0.0:9092,SASL_PLAINTEXT://0.0.0.0:9098
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/configs/kafka_server_jaas.conf"
    volumes:
      - kafka-2-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs
    restart: always

  kafka-3:
    image: 'confluentinc/cp-server:latest'
    container_name: kafka-3
    depends_on:
      - zookeeper
      - kafka-1
      - kafka-2
    ports:
      - '9096:9096'
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      ZOOKEEPER_SASL_ENABLED: 'false'
      KAFKA_JVM_PERFORMANCE_OPTS: "-Xms1G -Xmx2G"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9096,INTERNAL://kafka-3:9092,SASL_PLAINTEXT://kafka-3:9098
      KAFKA_SECURITY_INTER_BROKER_PROTOCOL: SASL_PLAINTEXT
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT,SASL_PLAINTEXT:SASL_PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9096,INTERNAL://0.0.0.0:9092,SASL_PLAINTEXT://0.0.0.0:9098
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/configs/kafka_server_jaas.conf"
    volumes:
      - kafka-3-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs
    restart: always

volumes:
  kafka-1-data:
  kafka-2-data:
  kafka-3-data:





-->Test by creating a test topic and produce some message and consume using the below command


sudo docker-compose exec --user root kafka-1 /bin/bash

kafka-topics --create --topic sasl-ssl-demo --bootstrap-server kafka-1:9092 --partitions 1 --replication-factor 1

kafka-topics --list --bootstrap-server kafka-1:9092




-->Produce using

kafka-console-producer --bootstrap-server kafka-1:9098 --topic sasl-plaintext-test --producer.config /etc/kafka/configs/client-sasl.properties


kafka-console-consumer --bootstrap-server kafka-1:9098 --topic sasl-ssl-demo --from-beginning --consumer.config /etc/kafka/configs/client-sasl.properties




----------------------------------------------------------------------SASL SSL------------------------------------------------------------------------------------




Implementation of SASL SSL in Docker Compose

cd /etc/kafka/configs

1) Create Client-1 SASL SSL Properties
echo "security.protocol=SASL_SSL" > client-sasl-ssl-1.properties
echo "sasl.mechanism=PLAIN" >> client-sasl-ssl-1.properties
echo "sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\" >> client-sasl-ssl-1.properties
echo "    username=\"nandan\" \\" >> client-sasl-ssl-1.properties
echo "    password=\"729999\";" >> client-sasl-ssl-1.properties
echo "ssl.truststore.location=/etc/kafka/secrets/client-1.truststore.jks" >> client-sasl-ssl-1.properties
echo "ssl.truststore.password=729999" >> client-sasl-ssl-1.properties
echo "ssl.keystore.location=/etc/kafka/secrets/client-1.keystore.jks" >> client-sasl-ssl-1.properties
echo "ssl.keystore.password=729999" >> client-sasl-ssl-1.properties
echo "ssl.key.password=729999" >> client-sasl-ssl-1.properties
echo "max.request.size=209715200" >> client-sasl-ssl.properties

2) Create Client-2 SASL SSL Properties
echo "security.protocol=SASL_SSL" > client-sasl-ssl-2.properties
echo "sasl.mechanism=PLAIN" >> client-sasl-ssl-2.properties
echo "sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\" >> client-sasl-ssl-2.properties
echo "    username=\"nandan\" \\" >> client-sasl-ssl-2.properties
echo "    password=\"729999\";" >> client-sasl-ssl-2.properties
echo "ssl.truststore.location=/etc/kafka/secrets/client-2.truststore.jks" >> client-sasl-ssl-2.properties
echo "ssl.truststore.password=729999" >> client-sasl-ssl-2.properties
echo "ssl.keystore.location=/etc/kafka/secrets/client-2.keystore.jks" >> client-sasl-ssl-2.properties
echo "ssl.keystore.password=729999" >> client-sasl-ssl-2.properties
echo "ssl.key.password=729999" >> client-sasl-ssl-2.properties
echo "max.request.size=209715200" >> client-sasl-ssl-2.properties

3) Create Client-3 SASL SSL Properties
echo "security.protocol=SASL_SSL" > client-sasl-ssl-3.properties
echo "sasl.mechanism=PLAIN" >> client-sasl-ssl-3.properties
echo "sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\" >> client-sasl-ssl-3.properties
echo "    username=\"nandan\" \\" >> client-sasl-ssl-3.properties
echo "    password=\"729999\";" >> client-sasl-ssl-3.properties
echo "ssl.truststore.location=/etc/kafka/secrets/client-3.truststore.jks" >> client-sasl-ssl-3.properties
echo "ssl.truststore.password=729999" >> client-sasl-ssl-3.properties
echo "ssl.keystore.location=/etc/kafka/secrets/client-3.keystore.jks" >> client-sasl-ssl-3.properties
echo "ssl.keystore.password=729999" >> client-sasl-ssl-3.properties
echo "ssl.key.password=729999" >> client-sasl-ssl-3.properties
echo "max.request.size=209715200" >> client-sasl-ssl-3.properties


version: '3.7'
services:
  zookeeper:
    image: 'confluentinc/cp-zookeeper'
    container_name: zookeeper
    ports:
      - '2182:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SASL_ENABLED: 'false'
    volumes:
      - /etc/kafka/data/zookeeper_data:/var/lib/zookeeper/data
      - /etc/kafka/data/zookeeper_log:/var/lib/zookeeper/log
    restart: always
  kafka-1:
    image: 'confluentinc/cp-server:latest'
    container_name: kafka-1
    depends_on:
      - zookeeper
    ports:
      - '9094:9094'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      ZOOKEEPER_SASL_ENABLED: 'false'
      KAFKA_JVM_PERFORMANCE_OPTS: "-Xms1G -Xmx2G"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9094,INTERNAL://kafka-1:9092,SASL_SSL://kafka-1:9098
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT,SASL_SSL:SASL_SSL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9094,INTERNAL://0.0.0.0:9092,SASL_SSL://0.0.0.0:9098
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/configs/kafka_server_jaas.conf"
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_SECURITY_INTER_BROKER_PROTOCOL: SASL_SSL
      KAFKA_SSL_KEYSTORE_FILENAME: kafka-1.keystore.jks
      KAFKA_SSL_KEY_CREDENTIALS: key-password.txt
      KAFKA_SSL_KEYSTORE_CREDENTIALS: keystore-password.txt
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: truststore-password.txt
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka-1.truststore.jks
      KAFKA_SSL_CLIENT_AUTH: 'required'
    volumes:
      - kafka-1-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs      
    restart: always
  kafka-2:
    image: 'confluentinc/cp-server:latest'
    container_name: kafka-2
    depends_on:
      - zookeeper
      - kafka-1
    ports:
      - '9095:9095'
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      ZOOKEEPER_SASL_ENABLED: 'false'
      KAFKA_JVM_PERFORMANCE_OPTS: "-Xms1G -Xmx2G"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9095,INTERNAL://kafka-2:9092,SASL_SSL://kafka-2:9098
      KAFKA_SECURITY_INTER_BROKER_PROTOCOL: SASL_SSL
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT,SASL_SSL:SASL_SSL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9095,INTERNAL://0.0.0.0:9092,SASL_SSL://0.0.0.0:9098
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/configs/kafka_server_jaas.conf"
      KAFKA_SSL_KEYSTORE_FILENAME: kafka-2.keystore.jks
      KAFKA_SSL_KEY_CREDENTIALS: key-password.txt
      KAFKA_SSL_KEYSTORE_CREDENTIALS: keystore-password.txt
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: truststore-password.txt
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka-2.truststore.jks
      KAFKA_SSL_CLIENT_AUTH: 'required'
    volumes:
      - kafka-2-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs
    restart: always
  kafka-3:
    image: 'confluentinc/cp-server:latest'
    container_name: kafka-3
    depends_on:
      - zookeeper
      - kafka-1
      - kafka-2
    ports:
      - '9096:9096'
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      ZOOKEEPER_SASL_ENABLED: 'false'
      KAFKA_JVM_PERFORMANCE_OPTS: "-Xms1G -Xmx2G"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9096,INTERNAL://kafka-3:9092,SASL_SSL://kafka-3:9098
      KAFKA_SECURITY_INTER_BROKER_PROTOCOL: SASL_SSL
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT,SASL_SSL:SASL_SSL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9096,INTERNAL://0.0.0.0:9092,SASL_SSL://0.0.0.0:9098
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/configs/kafka_server_jaas.conf"
      KAFKA_SSL_KEYSTORE_FILENAME: kafka-3.keystore.jks
      KAFKA_SSL_KEY_CREDENTIALS: key-password.txt
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka-3.truststore.jks
      KAFKA_SSL_CLIENT_AUTH: 'required'
    volumes:
      - kafka-3-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs
    restart: always
volumes:
  kafka-1-data:
  kafka-2-data:
  kafka-3-data:  

Test by creating a test topic and produce some message and consume using the below command

kafka-console-consumer --bootstrap-server kafka-1:9098 --topic sasl-test --from-beginning --consumer.config /etc/kafka/configs/client-sasl-ssl-1.properties



-----------------------------------------------------------------------------------------RBAC-----------------------------------------------------------------------------------------------



Implementation of RBAC using Hash Login in Docker Compose

cd /etc/kafka/secrets

1) Create Key Pair and Public Key
openssl genrsa -out tokenKeypair.pem 2048
openssl rsa -in tokenKeypair.pem -outform PEM -pubout -out tokenPublicKey.pem

2) Create Login Properties
echo "mds:mds" > ./login.properties


4) Create MDS Keystore snd Truststore with already created ca-cert
keytool -keystore mds.keystore.jks -alias localhost -validity 365 -genkey -keyalg RSA -dname "CN=localhost, OU=Streaming, O=Platformatory, L=Uttarahalli, ST=Karnataka, C=IN" -ext SAN=dns:localhost
keytool -keystore mds.keystore.jks -alias localhost -certreq -file mds-cert-file
echo subjectAltName = DNS:localhost >> mds-extfile.cnf
openssl x509 -req -CA ca-cert.pem -CAkey ca-key.pem -in mds-cert-file -out mds-cert-signed.pem -days 365 -CAcreateserial -extfile mds-extfile.cnf
keytool -keystore mds.keystore.jks -alias CARoot -import -file ca-cert.pem
keytool -keystore mds.keystore.jks -alias localhost -import -file mds-cert-signed.pem
openssl verify -CAfile ca-cert.pem mds-cert-signed.pem
#mds-cert-signed.pem: OK
keytool -importcert -file ca-cert.pem -alias CARoot -keystore mds.truststore.jks

5) Create Custom Dockerfile to use Confluent CLI within each container_name

a) For Kafka-1
cd ./kafka-1
Create Docker file
FROM confluentinc/cp-server:latest
RUN curl -sL --http1.1 https://cnfl.io/cli | sh -s -- latest -b /kafka-1
WORKDIR /kafka-1

b) For Kafka-2
cd ./kafka-2
Create Docker file
FROM confluentinc/cp-server:latest
RUN curl -sL --http1.1 https://cnfl.io/cli | sh -s -- latest -b /kafka-2
WORKDIR /kafka-2

c) For Kafka-3
cd ./kafka-3
Create Docker file
FROM confluentinc/cp-server:latest
RUN curl -sL --http1.1 https://cnfl.io/cli | sh -s -- latest -b /kafka-3
WORKDIR /kafka-3


6) Update Docker Compose and rebuild it


version: '3.7'
services:
  zookeeper:
    image: 'confluentinc/cp-zookeeper'
    container_name: zookeeper
    ports:
      - '2182:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - /etc/kafka/data/zookeeper_data:/var/lib/zookeeper/data
      - /etc/kafka/data/zookeeper_log:/var/lib/zookeeper/log
    restart: always
  kafka-1:
    build:
      context: .
      dockerfile: kafka-1/Dockerfile
    image: kafka-1-custom
    container_name: kafka-1
    depends_on:
      - zookeeper
    ports:
      - '9094:9094'
      - '9097:9097'
      - '8090:8090'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      ZOOKEEPER_SASL_ENABLED: 'false'
      KAFKA_JVM_PERFORMANCE_OPTS: "-Xms1G -Xmx2G"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9094,INTERNAL://kafka-1:9092,TOKEN://kafka-1:9097,SASL_PLAINTEXT://kafka-1:9064,SASL_SSL://kafka-1:9061
      KAFKA_SECURITY_INTER_BROKER_PROTOCOL: SASL_SSL
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN,OAUTHBEARER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:SASL_SSL,PLAINTEXT:PLAINTEXT,TOKEN:SASL_PLAINTEXT,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9094,INTERNAL://0.0.0.0:9092,TOKEN://0.0.0.0:9097,SASL_PLAINTEXT://0.0.0.0:9064,SASL_SSL://0.0.0.0:9061
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/configs/kafka_server_jaas.conf"
      KAFKA_SSL_KEYSTORE_FILENAME: kafka-1.keystore.jks
      KAFKA_SSL_KEY_CREDENTIALS: key-password.txt
      KAFKA_SSL_KEYSTORE_CREDENTIALS: keystore-password.txt
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: truststore-password.txt
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka-1.truststore.jks
      KAFKA_SSL_CLIENT_AUTH: 'required'
      KAFKA_SUPER_USERS: User:admin;User:mds;User:superUser;User:nandan
      KAFKA_CONFLUENT_METADATA_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_METADATA_SERVER_TOKEN_KEY_PATH: /etc/kafka/secrets/tokenKeypair.pem
      KAFKA_AUTHORIZER_CLASS_NAME: io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
      KAFKA_CONFLUENT_AUTHORIZER_ACCESS_RULE_PROVIDERS: CONFLUENT
      KAFKA_CONFLUENT_METADATA_SERVER_LISTENERS: https://0.0.0.0:8090
      KAFKA_CONFLUENT_METADATA_SERVER_ADVERTISED_LISTENERS: https://localhost:8090
      KAFKA_CONFLUENT_METADATA_SERVER_AUTHENTICATION_METHOD: BEARER
      KAFKA_CONFLUENT_METADATA_SERVER_USER_STORE: FILE
      KAFKA_CONFLUENT_METADATA_SERVER_USER_STORE_FILE_PATH: /etc/kafka/secrets/login.properties
      KAFKA_LISTENER_NAME_CLIENT_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_TOKEN_SASL_ENABLED_MECHANISMS: OAUTHBEARER
      KAFKA_LISTENER_NAME_TOKEN_OAUTHBEARER_SASL_JAAS_CONFIG: |
              org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required \
              publicKeyPath="/etc/kafka/secrets/tokenPublicKey.pem";
      KAFKA_LISTENER_NAME_TOKEN_OAUTHBEARER_SASL_SERVER_CALLBACK_HANDLER_CLASS: io.confluent.kafka.server.plugins.auth.token.TokenBearerValidatorCallbackHandler
      KAFKA_LISTENER_NAME_TOKEN_OAUTHBEARER_SASL_LOGIN_CALLBACK_HANDLER_CLASS: io.confluent.kafka.server.plugins.auth.token.TokenBearerServerLoginCallbackHandler
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_SECURITY_EVENT_LOGGER_EXPORTER_KAFKA_TOPIC_REPLICAS: 1
      KAFKA_LISTENER_NAME_INTERNAL_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_INTERNAL_PLAIN_SASL_JAAS_CONFIG: |
              org.apache.kafka.common.security.plain.PlainLoginModule required \
              username="nandan" \
              password="729999" \
              user_nandan="729999" \
              user_mds="mds";
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_TRUSTSTORE_LOCATION: /etc/kafka/secrets/mds.truststore.jks
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_TRUSTSTORE_PASSWORD: 729999
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_KEYSTORE_LOCATION: /etc/kafka/secrets/mds.keystore.jks
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_KEYSTORE_PASSWORD: 729999
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_KEY_PASSWORD: 729999
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_CIPHER_SUITES: ${SSL_CIPHER_SUITES}
      KAFKA_LISTENER_NAME_SSL_SSL_PRINCIPAL_MAPPING_RULES: RULE:^CN=([a-zA-Z0-9.]*).*$$/$$1/ , DEFAULT
      KAFKA_LISTENER_NAME_TOKEN_SSL_PRINCIPAL_MAPPING_RULES: RULE:^CN=([a-zA-Z0-9.]*).*$$/$$1/ , DEFAULT
      kAFKA_CONFLUENT_METADATA_SERVER_TOKEN_SIGNATURE_ALGORITHM: RS256
      PATH: "/home/appuser/bin:$PATH"
    volumes:
      - kafka-1-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs      
    restart: always
  kafka-2:
    build:
      context: .
      dockerfile: kafka-2/Dockerfile
    image: kafka-2-custom
    container_name: kafka-2
    depends_on:
      - zookeeper
      - kafka-1
    ports:
      - '9095:9095'
      - '9098:9098'
      - '8091:8091'
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      ZOOKEEPER_SASL_ENABLED: 'false'
      KAFKA_JVM_PERFORMANCE_OPTS: "-Xms1G -Xmx2G"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9095,INTERNAL://kafka-2:9092,TOKEN://kafka-2:9098,SASL_PLAINTEXT://kafka-2:9065,SASL_SSL://kafka-2:9062
      KAFKA_SECURITY_INTER_BROKER_PROTOCOL: SASL_SSL
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN,OAUTHBEARER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:SASL_SSL,PLAINTEXT:PLAINTEXT,TOKEN:SASL_PLAINTEXT,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9095,INTERNAL://0.0.0.0:9092,TOKEN://0.0.0.0:9098,SASL_PLAINTEXT://0.0.0.0:9065,SASL_SSL://0.0.0.0:9062
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/configs/kafka_server_jaas.conf"
      KAFKA_SSL_KEYSTORE_FILENAME: kafka-2.keystore.jks
      KAFKA_SSL_KEY_CREDENTIALS: key-password.txt
      KAFKA_SSL_KEYSTORE_CREDENTIALS: keystore-password.txt
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: truststore-password.txt
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka-2.truststore.jks
      KAFKA_SSL_CLIENT_AUTH: 'required'
      KAFKA_SUPER_USERS: User:admin;User:mds;User:superUser;User:nandan
      KAFKA_CONFLUENT_METADATA_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_METADATA_SERVER_TOKEN_KEY_PATH: /etc/kafka/secrets/tokenKeypair.pem
      KAFKA_AUTHORIZER_CLASS_NAME: io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
      KAFKA_CONFLUENT_AUTHORIZER_ACCESS_RULE_PROVIDERS: CONFLUENT
      KAFKA_CONFLUENT_METADATA_SERVER_LISTENERS: https://0.0.0.0:8091
      KAFKA_CONFLUENT_METADATA_SERVER_ADVERTISED_LISTENERS: https://localhost:8091
      KAFKA_CONFLUENT_METADATA_SERVER_AUTHENTICATION_METHOD: BEARER
      KAFKA_CONFLUENT_METADATA_SERVER_USER_STORE: FILE
      KAFKA_CONFLUENT_METADATA_SERVER_USER_STORE_FILE_PATH: /etc/kafka/secrets/login.properties
      KAFKA_LISTENER_NAME_TOKEN_SASL_ENABLED_MECHANISMS: OAUTHBEARER
      KAFKA_LISTENER_NAME_TOKEN_OAUTHBEARER_SASL_JAAS_CONFIG: |
              org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required \
              publicKeyPath="/etc/kafka/secrets/tokenPublicKey.pem";
      KAFKA_LISTENER_NAME_TOKEN_OAUTHBEARER_SASL_SERVER_CALLBACK_HANDLER_CLASS: io.confluent.kafka.server.plugins.auth.token.TokenBearerValidatorCallbackHandler
      KAFKA_LISTENER_NAME_TOKEN_OAUTHBEARER_SASL_LOGIN_CALLBACK_HANDLER_CLASS: io.confluent.kafka.server.plugins.auth.token.TokenBearerServerLoginCallbackHandler
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_SECURITY_EVENT_LOGGER_EXPORTER_KAFKA_TOPIC_REPLICAS: 1
      KAFKA_LISTENER_NAME_INTERNAL_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_INTERNAL_PLAIN_SASL_JAAS_CONFIG: |
              org.apache.kafka.common.security.plain.PlainLoginModule required \
              username="nandan" \
              password="729999" \
              user_nandan="729999" \
              user_mds="mds";
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_TRUSTSTORE_LOCATION: /etc/kafka/secrets/mds.truststore.jks
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_TRUSTSTORE_PASSWORD: 729999
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_KEYSTORE_LOCATION: /etc/kafka/secrets/mds.keystore.jks
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_KEYSTORE_PASSWORD: 729999
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_KEY_PASSWORD: 729999
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_CIPHER_SUITES: ${SSL_CIPHER_SUITES}
      KAFKA_LISTENER_NAME_SSL_SSL_PRINCIPAL_MAPPING_RULES: RULE:^CN=([a-zA-Z0-9.]*).*$$/$$1/ , DEFAULT
      KAFKA_LISTENER_NAME_TOKEN_SSL_PRINCIPAL_MAPPING_RULES: RULE:^CN=([a-zA-Z0-9.]*).*$$/$$1/ , DEFAULT
      kAFKA_CONFLUENT_METADATA_SERVER_TOKEN_SIGNATURE_ALGORITHM: RS256
      PATH: "/home/appuser/bin:$PATH"
    volumes:
      - kafka-2-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs
    restart: always
  kafka-3:
    build:
      context: .
      dockerfile: kafka-3/Dockerfile
    image: kafka-3-custom
    container_name: kafka-3
    depends_on:
      - zookeeper
      - kafka-1
      - kafka-2
    ports:
      - '9096:9096'
      - '9099:9099'
      - '8092:8092'
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      ZOOKEEPER_SASL_ENABLED: 'false'
      KAFKA_JVM_PERFORMANCE_OPTS: "-Xms1G -Xmx2G"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9096,INTERNAL://kafka-3:9092,TOKEN://kafka-3:9099,SASL_PLAINTEXT://kafka-3:9066,SASL_SSL://kafka-3:9063
      KAFKA_SECURITY_INTER_BROKER_PROTOCOL: SASL_SSL
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN,OAUTHBEARER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:SASL_SSL,PLAINTEXT:PLAINTEXT,TOKEN:SASL_PLAINTEXT,SASL_PLAINTEXT:SASL_PLAINTEXT,SASL_SSL:SASL_SSL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9096,INTERNAL://0.0.0.0:9092,TOKEN://0.0.0.0:9099,SASL_PLAINTEXT://0.0.0.0:9066,SASL_SSL://0.0.0.0:9063
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/configs/kafka_server_jaas.conf"
      KAFKA_SSL_KEYSTORE_FILENAME: kafka-3.keystore.jks
      KAFKA_SSL_KEY_CREDENTIALS: key-password.txt
      KAFKA_SSL_KEYSTORE_CREDENTIALS: keystore-password.txt
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: truststore-password.txt
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka-3.truststore.jks
      KAFKA_SSL_CLIENT_AUTH: 'required'
      KAFKA_SUPER_USERS: User:admin;User:mds;User:superUser;User:nandan
      KAFKA_CONFLUENT_METADATA_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_METADATA_SERVER_TOKEN_KEY_PATH: /etc/kafka/secrets/tokenKeypair.pem
      KAFKA_AUTHORIZER_CLASS_NAME: io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
      KAFKA_CONFLUENT_AUTHORIZER_ACCESS_RULE_PROVIDERS: CONFLUENT
      KAFKA_CONFLUENT_METADATA_SERVER_LISTENERS: https://0.0.0.0:8092
      KAFKA_CONFLUENT_METADATA_SERVER_ADVERTISED_LISTENERS: https://localhost:8092
      KAFKA_CONFLUENT_METADATA_SERVER_AUTHENTICATION_METHOD: BEARER
      KAFKA_CONFLUENT_METADATA_SERVER_USER_STORE: FILE
      KAFKA_CONFLUENT_METADATA_SERVER_USER_STORE_FILE_PATH: /etc/kafka/secrets/login.properties
      KAFKA_LISTENER_NAME_TOKEN_SASL_ENABLED_MECHANISMS: OAUTHBEARER
      KAFKA_LISTENER_NAME_TOKEN_OAUTHBEARER_SASL_JAAS_CONFIG: |
              org.apache.kafka.common.security.oauthbearer.OAuthBearerLoginModule required \
              publicKeyPath="/etc/kafka/secrets/tokenPublicKey.pem";
      KAFKA_LISTENER_NAME_TOKEN_OAUTHBEARER_SASL_SERVER_CALLBACK_HANDLER_CLASS: io.confluent.kafka.server.plugins.auth.token.TokenBearerValidatorCallbackHandler
      KAFKA_LISTENER_NAME_TOKEN_OAUTHBEARER_SASL_LOGIN_CALLBACK_HANDLER_CLASS: io.confluent.kafka.server.plugins.auth.token.TokenBearerServerLoginCallbackHandler
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_SECURITY_EVENT_LOGGER_EXPORTER_KAFKA_TOPIC_REPLICAS: 1
      KAFKA_LISTENER_NAME_INTERNAL_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_NAME_INTERNAL_PLAIN_SASL_JAAS_CONFIG: |
              org.apache.kafka.common.security.plain.PlainLoginModule required \
              username="nandan" \
              password="729999" \
              user_nandan="729999" \
              user_mds="mds";
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_TRUSTSTORE_LOCATION: /etc/kafka/secrets/mds.truststore.jks
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_TRUSTSTORE_PASSWORD: certificate
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_KEYSTORE_LOCATION: /etc/kafka/secrets/mds.keystore.jks
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_KEYSTORE_PASSWORD: certificate
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_KEY_PASSWORD: certificate
      KAFKA_CONFLUENT_METADATA_SERVER_SSL_CIPHER_SUITES: ${SSL_CIPHER_SUITES}
      KAFKA_LISTENER_NAME_SSL_SSL_PRINCIPAL_MAPPING_RULES: RULE:^CN=([a-zA-Z0-9.]*).*$$/$$1/ , DEFAULT
      KAFKA_LISTENER_NAME_TOKEN_SSL_PRINCIPAL_MAPPING_RULES: RULE:^CN=([a-zA-Z0-9.]*).*$$/$$1/ , DEFAULT
      kAFKA_CONFLUENT_METADATA_SERVER_TOKEN_SIGNATURE_ALGORITHM: RS256
      PATH: "/home/appuser/bin:$PATH"
    volumes:
      - kafka-3-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs
    restart: always
volumes:
  kafka-1-data:
  kafka-2-data:
  kafka-3-data:








































































































































--------------------------------------------------------------------------------------------Backup codes----------------------------------------------------------------------------------------



version: '3.7'
services:
  zookeeper:
    image: 'confluentinc/cp-zookeeper'
    container_name: zookeeper
    ports:
      - '2182:2181'
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      ZOOKEEPER_SASL_ENABLED: 'false'
    volumes:
      - /etc/kafka/data/zookeeper_data:/var/lib/zookeeper/data
      - /etc/kafka/data/zookeeper_log:/var/lib/zookeeper/log
    restart: always
  kafka-1:
    image: 'confluentinc/cp-server:latest'
    container_name: kafka-1
    depends_on:
      - zookeeper
    ports:
      - '9094:9094'
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      ZOOKEEPER_SASL_ENABLED: 'false'
      KAFKA_JVM_PERFORMANCE_OPTS: "-Xms1G -Xmx2G"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9094,INTERNAL://kafka-1:9092,SASL_SSL://kafka-1:9098
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT,SASL_SSL:SASL_SSL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9094,INTERNAL://0.0.0.0:9092,SASL_SSL://0.0.0.0:9098
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/configs/kafka_server_jaas.conf"
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_SECURITY_INTER_BROKER_PROTOCOL: SASL_SSL
      KAFKA_SSL_KEYSTORE_FILENAME: kafka-1.keystore.jks
      KAFKA_SSL_KEY_CREDENTIALS: key-password.txt
      KAFKA_SSL_KEYSTORE_CREDENTIALS: keystore-password.txt
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: truststore-password.txt
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka-1.truststore.jks
      KAFKA_SSL_CLIENT_AUTH: 'required'
    volumes:
      - kafka-1-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs      
    restart: always
  kafka-2:
    image: 'confluentinc/cp-server:latest'
    container_name: kafka-2
    depends_on:
      - zookeeper
      - kafka-1
    ports:
      - '9095:9095'
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      ZOOKEEPER_SASL_ENABLED: 'false'
      KAFKA_JVM_PERFORMANCE_OPTS: "-Xms1G -Xmx2G"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9095,INTERNAL://kafka-2:9092,SASL_SSL://kafka-2:9098
      KAFKA_SECURITY_INTER_BROKER_PROTOCOL: SASL_SSL
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT,SASL_SSL:SASL_SSL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9095,INTERNAL://0.0.0.0:9092,SASL_SSL://0.0.0.0:9098
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/configs/kafka_server_jaas.conf"
      KAFKA_SSL_KEYSTORE_FILENAME: kafka-2.keystore.jks
      KAFKA_SSL_KEY_CREDENTIALS: key-password.txt
      KAFKA_SSL_KEYSTORE_CREDENTIALS: keystore-password.txt
      KAFKA_SSL_TRUSTSTORE_CREDENTIALS: truststore-password.txt
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka-2.truststore.jks
      KAFKA_SSL_CLIENT_AUTH: 'required'
    volumes:
      - kafka-2-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs
    restart: always
  kafka-3:
    image: 'confluentinc/cp-server:latest'
    container_name: kafka-3
    depends_on:
      - zookeeper
      - kafka-1
      - kafka-2
    ports:
      - '9096:9096'
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      ZOOKEEPER_SASL_ENABLED: 'false'
      KAFKA_JVM_PERFORMANCE_OPTS: "-Xms1G -Xmx2G"
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9096,INTERNAL://kafka-3:9092,SASL_SSL://kafka-3:9098
      KAFKA_SECURITY_INTER_BROKER_PROTOCOL: SASL_SSL
      KAFKA_SASL_MECHANISM_INTER_BROKER_PROTOCOL: PLAIN
      KAFKA_SASL_ENABLED_MECHANISMS: PLAIN
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,PLAINTEXT:PLAINTEXT,SASL_SSL:SASL_SSL
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 2
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9096,INTERNAL://0.0.0.0:9092,SASL_SSL://0.0.0.0:9098
      KAFKA_OPTS: "-Djava.security.auth.login.config=/etc/kafka/configs/kafka_server_jaas.conf"
      KAFKA_SSL_KEYSTORE_FILENAME: kafka-3.keystore.jks
      KAFKA_SSL_KEY_CREDENTIALS: key-password.txt
      KAFKA_SSL_TRUSTSTORE_FILENAME: kafka-3.truststore.jks
      KAFKA_SSL_CLIENT_AUTH: 'required'
    volumes:
      - kafka-3-data:/var/lib/kafka/data
      - ./certs:/etc/kafka/secrets
      - ./configs:/etc/kafka/configs
    restart: always
volumes:
  kafka-1-data:
  kafka-2-data:
  kafka-3-data:  














   


























1) CA Certificate
openssl req -new -x509 -keyout ca-key.pem -out ca-cert.pem -days 365 -subj "/C=IN/ST=Karnataka/L=Uttarahalli/O=Platformatory/OU=Kafka/CN=Platformatory-CA"

2) Client-1 Keystore and Truststore
a. keytool -keystore client-1.keystore.jks -alias kafka-1 -validity 365 -genkey -keyalg RSA -dname "CN=kafka1, OU=Kafka, O=Platformatory, L=Uttarahalli, ST=Karnataka, C=IN"
b. keytool -keystore client-1.keystore.jks -alias kafka-1 -certreq -file client-1-cert-file
c. openssl x509 -req -CA ca-cert.pem -CAkey ca-key.pem -in client-1-cert-file -out client-1-cert-signed.pem -days 365 -CAcreateserial
d. keytool -keystore client-1.keystore.jks -alias CARoot -import -file ca-cert.pem
e. keytool -keystore client-1.keystore.jks -alias kafka-1 -import -file client-1-cert-signed.pem
f. openssl verify -CAfile ca-cert.pem client-1-cert-signed.pem
#client-cert-signed.pem: OK
g. keytool -importcert -file ca-cert.pem -alias CARoot -keystore client-1.truststore.jks

3) Kafka-1 Keystore and Truststore
a. keytool -keystore kafka-1.keystore.jks -alias kafka-1 -validity 365 -genkey -keyalg RSA -dname "CN=kafka1, OU=Kafka, O=Platformatory, L=Uttarahalli, ST=Karnataka, C=IN"
b. keytool -keystore kafka-1.keystore.jks -alias kafka-1 -certreq -file kafka-1-cert-file
c. openssl x509 -req -CA ca-cert.pem -CAkey ca-key.pem -in kafka-1-cert-file -out kafka-1-cert-signed.pem -days 365 -CAcreateserial
d. keytool -keystore kafka-1.keystore.jks -alias CARoot -import -file ca-cert.pem
e. keytool -keystore kafka-1.keystore.jks -alias kafka-1 -import -file kafka-1-cert-signed.pem
f. openssl verify -CAfile ca-cert.pem kafka-1-cert-signed.pem
#kafka-1-cert-signed.pem: OK
g. keytool -importcert -file ca-cert.pem -alias CARoot -keystore kafka-1.truststore.jks





kafka-acls --authorizer-properties zookeeper.connect=zookeeper1:2181 \
  --add --allow-principal User:kafka1 \
  --operation ALL \
  --topic _confluent-metadata-auth



















    kafka2:
    image: confluentinc/cp-server:7.4.0
    hostname: kafka2
    container_name: kafka2
    depends_on:
      - zookeeper1
    command: kafka-server-start /etc/kafka/server.properties
    environment:
      EXTRA_ARGS: -javaagent:/usr/share/jmx-exporter/jmx_prometheus_javaagent-0.18.0.jar=9102:/usr/share/jmx-exporter/kafka_broker.yml
    volumes:
    - ./kafka2:/etc/kafka
    - ./jmx-exporter:/usr/share/jmx-exporter
    - kafka2data:/var/lib/kafka/data
    deploy:
      resources:
        limits:
          cpus: "1.5"
          memory: 1536M
